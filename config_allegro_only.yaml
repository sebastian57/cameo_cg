# Chemtrain Clean Code Base - Configuration Template
# Based on config_allegro_exp2.yaml with new features added

seed: 193749
debug_priors: false

model_context: "allegro_cg_protein_2g4q4z5k"
protein_name: "2g4q4z5k"
model_id: "allegro_only"

data:
  #path: "data_prep/datasets/4zohB01_320K_kcalmol_1bead_notnorm_aggforce.npz"
  path: "data_prep/datasets/2g4q4z5k_320K_kcalmol_1bead_notnorm_aggforce.npz"
  max_frames: 10000  # Maximum frames to use from dataset (null = use all)

preprocessing:
  buffer_multiplier: 2.0  # Multiplier for box buffer (box = max_range + 2 * buffer_multiplier * cutoff)
  park_multiplier: 0.95   # Parking location for padded atoms (fraction of box extent)

model:
  use_priors: false  

  allegro_size: "default"  # Options: "default", "large", "med"

  cutoff: 10.0  # Neighbor list cutoff distance
  dr_threshold: 1.0  # Neighbor list rebuild threshold

 
  allegro:  
    max_ell: 2
    num_layers: 3
    n_radial_basis: 24
    envelope_p: 6
    embed_n_hidden: [64, 128]
    species_embed: 22
    mlp_n_hidden: 96
    mlp_n_layers: 3
    avg_num_neighbors: 21

  allegro_large: 
    num_layers: 4
    n_radial_basis: 36
    envelope_p: 6
    embed_n_hidden: [96, 192]
    species_embed: 32
    mlp_n_hidden: 192
    mlp_n_layers: 3
    avg_num_neighbors: 8

  allegro_med: 
    num_types: 22
    max_ell: 2
    num_layers: 3
    n_radial_basis: 18
    envelope_p: 6
    embed_n_hidden: [64, 128]
    species_embed: 32
    mlp_n_hidden: 128
    mlp_n_layers: 2
    avg_num_neighbors: 12

  priors:
    weights:
      bond: 0.5       
      angle: 0.25      
      dihedral: 0.25   
      repulsive: 1.0   

    r0: 3.8375435  # Equilibrium bond length
    kr: 154.50629422490843  # Bond force constant

    a: [-0.02086511, -0.36341857, -0.50767196, 0.09906029, 0.8319552, -0.00770968, -0.13320648, -1.14116021, 0.18145372, -0.55828783]
    b: [0.67521775, 0.14797897, -0.12157499, -0.8289584, 0.17162394, 0.48646108, 0.56000923, -0.13905056, -0.896762, -1.20144583]

    # Legacy angle parameters (unused, kept for potential future use)
    theta0: 1.8335507
    k_theta: 8.271271714604836

    epsilon: 1.0  # Repulsive energy scale
    sigma: 3.0    # Repulsive length scale
    # ========================================================================

    # Dihedral parameters (periodic)
    k_dih: [0.47037187851535034, 0.9495107825945361]
    gamma_dih: [1.3759673785180062, 1.6211158177819938]

optimizer:
  grad_clip: 2.0  # Global gradient clipping

  adabelief:
    lr: 5.0e-2
    peak_lr: 0.1
    end_lr: 5.0e-3
    warmup_epochs: 15
    decay_steps: 50000
    beta1: 0.95
    beta2: 0.999
    eps: 1.0e-8
    grad_clip: 5.0
    weight_decay: 2.0e-5

  yogi:
    lr: 1.0e-3
    peak_lr: 5.0e-3
    end_lr: 5.0e-4
    warmup_epochs: 5
    decay_steps: 10000
    beta1: 0.9
    beta2: 0.999
    grad_clip: 4.0
    eps: 1.0e-6
    weight_decay: 2.0e-5

  adam:
    lr: 0.002
    peak_lr: 0.005
    end_lr: 0.001
    warmup_epochs: 0
    decay_steps: 160
    beta1: 0.99
    beta2: 0.999

  lion:
    lr: 0.002
    peak_lr: 0.005
    end_lr: 0.0005
    warmup_epochs: 0
    decay_steps: 50
    beta1: 0.9
    beta2: 0.99

  polyak_sgd:
    f_star: 0.0
    eps: 1.0e-8
    warmup_epochs: 5
    peak_lr: 0.002
    end_lr: 0.0002
    decay_steps: 200

  fromage:
    lr: 2.0e-4
    epochs: 10
    grad_clip: 1.0

ensemble:
  enabled: false              # Set to true to enable ensemble training
  n_models: 5                 # Number of models to train
  base_seed: 42               # Base seed (models use seeds: base_seed, base_seed+1, ...)
  save_all_models: false      # If true, save all models; if false, only save best

training:
  pretrain_prior: false  # Enable prior-only pre-training before full training
  pretrain_prior_min_steps: 10  # Minimum LBFGS steps before convergence check
  pretrain_prior_max_steps: 200  # Maximum LBFGS iterations
  pretrain_prior_tol_grad: 1.0e-6  # Gradient tolerance for convergence

  stage1_optimizer: "adabelief"  # First training stage optimizer
  stage2_optimizer: "yogi"  # Second training stage (fine-tuning)

  epochs_adabelief: 100 # looks like 30 epochs in 4 hours with 10k structures and moderate model size / cutoff
  epochs_yogi: 50  # Set to 0 to skip this stage

  val_fraction: 0.1  # Validation set fraction
  batch_per_device: 2  # Batch size per GPU
  batch_cache: 10  # Number of batches to cache

  gammas:
    F: 1.0  # Force weight
    U: 0.0  # Energy weight (usually 0 for force matching)

  # Checkpointing
  checkpoint_freq: 25  # Save checkpoint every N epochs (0 = only at end)
  checkpoint_path: "./checkpoints_allegro"

  # Export
  export_path: "./exported_models"


