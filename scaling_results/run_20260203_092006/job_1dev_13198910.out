============================================================
Module Environment
============================================================

============================================================
SLURM Job Configuration
============================================================
Config file:    config_timing_test.yaml
Job ID:         13198910
Nodes:          1
Tasks/node:     1 (1 process per node)
GPUs per node:  4 (pmap distributes across local GPUs)
Total GPUs:     4
CUDA_HOME:      /p/software/juwelsbooster/stages/2025/software/CUDA/12
CUDA_VISIBLE:   0
============================================================
Verifying GPU allocation per node...
Host=jwb0701.juwels CUDA_VISIBLE_DEVICES=0
GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-4af2029c-ef22-1004-7f1d-91a5a78a0bf5)
GPU 1: NVIDIA A100-SXM4-40GB (UUID: GPU-9fd0e1c3-dd7e-66c4-6b72-ec180cccd3bb)
GPU 2: NVIDIA A100-SXM4-40GB (UUID: GPU-53c7ec81-0f0d-cf8a-6567-381adabdb02d)
GPU 3: NVIDIA A100-SXM4-40GB (UUID: GPU-a6dc84c1-3661-d477-feb5-aa3ffc9b3264)
Submit directory: /p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base
Training script:  /p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train.py
============================================================
Starting training with 1 node(s), 4 GPUs each...
Log file: train_allegro_13198910.log
============================================================
[Data] Resolved relative path: data_prep/datasets/2g4q4z5k_320K_kcalmol_1bead_notnorm_aggforce.npz -> /p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/data_prep/datasets/2g4q4z5k_320K_kcalmol_1bead_notnorm_aggforce.npz
[Data] N_max: 99
[Data] Species: [14 20 16 16 13 15  7 20 21 11 16  3  2 12  2  6  0 12  6 15 12  6  6  8
  5  3  0  1 16 12  0  7  7 10 17 12 11 16 14 12 13 12  1 12 11  1 16 17
 20 11 21  6 11 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1]
[Data] Total frames: 10000
[Data] [Preprocessing] Computed box: [398.13    442.15002 444.7    ]
[Data] [Preprocessing] R_shift: [188.9     161.41    156.73001]
[Model] Using Allegro size: default
[Model] Detected 22 unique species
[Model] Using Allegro config size: default
[Model] Mode: Prior + Allegro
[Model] Prior weights: {'bond': 0.5, 'angle': 0.25, 'dihedral': 0.25, 'repulsive': 1.0}
[Model] Initialized: CombinedModel(mode=Prior+Allegro, N_max=99)
[Data] [Split] Training: 8000, Validation: 2000
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:183: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return asarray(x, dtype=self.dtype)
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:213: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in sum is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return _reduction(a, "sum", np.sum, lax.add, 0, preproc=_cast_to_numeric,
[Training] Initialized model with seed=8951
[Training] Applied NumpyDataLoader patch
[Training] 
============================================================
[Training] Prior Pre-Training (LBFGS, max_steps=200)
[Training] ============================================================
[Training] [LBFGS] Starting optimization...
[Training] [LBFGS] Completed: 200 steps
[Training] [LBFGS] Final loss: 2.690221e+02
[Training] [LBFGS] Grad norm: 5.939185e-01 (tol=1.000000e-06)
[Training] [LBFGS] Converged: False
[Training] 
[LBFGS] Fitted parameters:
[Training]   a: [ 3.0022867  -3.571617   -2.525056    0.76882803  3.5351863   1.0975847
 -1.1064873  -4.3567653  -1.0390962  -4.2006364 ]
[Training]   b: [ 1.9151835  3.6027696 -3.164523  -2.344312  -1.023896   3.6180687
  1.6900855  1.3664566 -4.8307457 -3.3700397]
[Training]   epsilon: 0.706392
[Training]   gamma_dih: [-6.201924 -7.661019]
[Training]   k_dih: [ 6.024526  -1.0554717]
[Training]   kr: 288.316162
[Training]   r0: 3.839072
[Training]   sigma: -3.067905
[Training] 
============================================================
[Training] Training Stage: ADABELIEF (20 epochs, starting from 0)
[Training] ============================================================
[Single-process mode] Local devices: 1
[Rank 0] Local devices: [CudaDevice(id=0)]
Capping edges and triplets. Beware of overflow, which is currently not being detected.
Estimated max. 3488 edges.
[Allegro] Use two atom species for oxygen and hydroge.
[Allegro] Use default mask
Use a custom scatter implementation
Out irreps 300x0e+556x1o+556x2e
Irreps after layer are 128x1o+128x2e
Use a custom scatter implementation
Out irreps 256x0e
Irreps after layer are Irreps()
[Allegro] Use two atom species for oxygen and hydroge.
[Allegro] Use default mask
Use a custom scatter implementation
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Out irreps 300x0e+556x1o+556x2e
Irreps after layer are 128x1o+128x2e
Use a custom scatter implementation
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Out irreps 256x0e
Irreps after layer are Irreps()
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:183: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return asarray(x, dtype=self.dtype)
[Force] Found precomputed forces.
