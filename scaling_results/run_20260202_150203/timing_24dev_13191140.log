[Model] Using Allegro size: default
[Model] Using Allegro size: default
[Model] Using Allegro size: default
[Model] Using Allegro size: default
[Model] Using Allegro size: default
[Model] Detected 18 unique species
[Model] Using Allegro config size: default
[Model] Using Allegro size: default
[Model] Detected 18 unique species
[Model] Using Allegro config size: default
[Model] Detected 18 unique species
[Model] Using Allegro config size: default
[Model] Detected 18 unique species
[Model] Using Allegro config size: default
[Model] Detected 18 unique species
[Model] Using Allegro config size: default
[Model] Detected 18 unique species
[Model] Using Allegro config size: default
[Model] Mode: Prior + Allegro
[Model] Prior weights: {'bond': 0.5, 'angle': 0.25, 'dihedral': 0.25, 'repulsive': 1.0}
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
[Model] Mode: Prior + Allegro
[Model] Prior weights: {'bond': 0.5, 'angle': 0.25, 'dihedral': 0.25, 'repulsive': 1.0}
[Model] Mode: Prior + Allegro
[Model] Prior weights: {'bond': 0.5, 'angle': 0.25, 'dihedral': 0.25, 'repulsive': 1.0}
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:183: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return asarray(x, dtype=self.dtype)
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:213: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in sum is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return _reduction(a, "sum", np.sum, lax.add, 0, preproc=_cast_to_numeric,
[Model] Mode: Prior + Allegro
[Model] Prior weights: {'bond': 0.5, 'angle': 0.25, 'dihedral': 0.25, 'repulsive': 1.0}
[Model] Mode: Prior + Allegro
[Model] Prior weights: {'bond': 0.5, 'angle': 0.25, 'dihedral': 0.25, 'repulsive': 1.0}
[Model] Mode: Prior + Allegro
[Model] Prior weights: {'bond': 0.5, 'angle': 0.25, 'dihedral': 0.25, 'repulsive': 1.0}
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:183: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return asarray(x, dtype=self.dtype)
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:213: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in sum is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return _reduction(a, "sum", np.sum, lax.add, 0, preproc=_cast_to_numeric,
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:183: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return asarray(x, dtype=self.dtype)
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:213: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in sum is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return _reduction(a, "sum", np.sum, lax.add, 0, preproc=_cast_to_numeric,
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:183: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return asarray(x, dtype=self.dtype)
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:213: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in sum is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return _reduction(a, "sum", np.sum, lax.add, 0, preproc=_cast_to_numeric,
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:183: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return asarray(x, dtype=self.dtype)
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:213: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in sum is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return _reduction(a, "sum", np.sum, lax.add, 0, preproc=_cast_to_numeric,
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:183: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return asarray(x, dtype=self.dtype)
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:213: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in sum is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return _reduction(a, "sum", np.sum, lax.add, 0, preproc=_cast_to_numeric,
[Training] Initialized model with seed=193749
[Training] Applied NumpyDataLoader patch
[Training] 
============================================================
[Training] Training Stage: ADABELIEF (3 epochs, starting from 0)
[Training] ============================================================
/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/chemtrain/trainers/base.py:957: UserWarning: Batch size 384 does not divide the number of observations 2250. Trainer will skip 330 samples for state training
  warnings.warn(
/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/chemtrain/trainers/base.py:957: UserWarning: Batch size 240 does not divide the number of observations 250. Trainer will skip 10 samples for state validation
  warnings.warn(
[Training] Initialized model with seed=193749
[Training] Applied NumpyDataLoader patch
[Training] 
============================================================
[Training] Training Stage: ADABELIEF (3 epochs, starting from 0)
[Training] ============================================================
[Training] Initialized model with seed=193749
[Training] Applied NumpyDataLoader patch
[Training] 
============================================================
[Training] Training Stage: ADABELIEF (3 epochs, starting from 0)
[Training] ============================================================
/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/chemtrain/trainers/base.py:957: UserWarning: Batch size 384 does not divide the number of observations 2250. Trainer will skip 330 samples for state training
  warnings.warn(
[Training] Initialized model with seed=193749
[Training] Applied NumpyDataLoader patch
[Training] 
============================================================
[Training] Training Stage: ADABELIEF (3 epochs, starting from 0)
[Training] ============================================================
/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/chemtrain/trainers/base.py:957: UserWarning: Batch size 240 does not divide the number of observations 250. Trainer will skip 10 samples for state validation
  warnings.warn(
[Training] Initialized model with seed=193749
[Training] Applied NumpyDataLoader patch
[Training] 
============================================================
[Training] Training Stage: ADABELIEF (3 epochs, starting from 0)
[Training] ============================================================
/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/chemtrain/trainers/base.py:957: UserWarning: Batch size 384 does not divide the number of observations 2250. Trainer will skip 330 samples for state training
  warnings.warn(
/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/chemtrain/trainers/base.py:957: UserWarning: Batch size 240 does not divide the number of observations 250. Trainer will skip 10 samples for state validation
  warnings.warn(
/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/chemtrain/trainers/base.py:957: UserWarning: Batch size 384 does not divide the number of observations 2250. Trainer will skip 330 samples for state training
  warnings.warn(
/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/chemtrain/trainers/base.py:957: UserWarning: Batch size 240 does not divide the number of observations 250. Trainer will skip 10 samples for state validation
  warnings.warn(
[Training] Initialized model with seed=193749
[Training] Applied NumpyDataLoader patch
[Training] 
============================================================
[Training] Training Stage: ADABELIEF (3 epochs, starting from 0)
[Training] ============================================================
/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/chemtrain/trainers/base.py:957: UserWarning: Batch size 384 does not divide the number of observations 2250. Trainer will skip 330 samples for state training
  warnings.warn(
/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/chemtrain/trainers/base.py:957: UserWarning: Batch size 240 does not divide the number of observations 250. Trainer will skip 10 samples for state validation
  warnings.warn(
/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/chemtrain/trainers/base.py:957: UserWarning: Batch size 384 does not divide the number of observations 2250. Trainer will skip 330 samples for state training
  warnings.warn(
/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/chemtrain/trainers/base.py:957: UserWarning: Batch size 240 does not divide the number of observations 250. Trainer will skip 10 samples for state validation
  warnings.warn(
[Scaling] Node 5/6: local_device_ids = [0, 1, 2, 3]
[Scaling] Node 5: CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb1159.juwels:29540
[Scaling] Actual processes: 6
[Rank 5/6] JAX distributed initialized
[Rank 5] Local GPUs: 4, Total GPUs: 24
[Rank 5] Local devices: [CudaDevice(id=20), CudaDevice(id=21), CudaDevice(id=22), CudaDevice(id=23)]
[Rank 5] All devices: [CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2), CudaDevice(id=3), CudaDevice(id=4), CudaDevice(id=5), CudaDevice(id=6), CudaDevice(id=7), CudaDevice(id=8), CudaDevice(id=9), CudaDevice(id=10), CudaDevice(id=11), CudaDevice(id=12), CudaDevice(id=13), CudaDevice(id=14), CudaDevice(id=15), CudaDevice(id=16), CudaDevice(id=17), CudaDevice(id=18), CudaDevice(id=19), CudaDevice(id=20), CudaDevice(id=21), CudaDevice(id=22), CudaDevice(id=23)]
============================================================
SCALING TEST
============================================================
Devices: 24
Distributed: True
Rank: 5/6
Local devices: 4
Global devices: 24
============================================================
Capping edges and triplets. Beware of overflow, which is currently not being detected.
Estimated max. 900 edges.
[Allegro] Use two atom species for oxygen and hydroge.
[Allegro] Use default mask
Use a custom scatter implementation
Out irreps 300x0e+556x1o+256x1e+556x2e+384x2o+512x3o+256x3e
Irreps after layer are 128x1o+128x1e+128x2e+128x2o+128x3o+128x3e
Use a custom scatter implementation
Out irreps 256x0e+768x1o+896x2e
Irreps after layer are 128x1o+128x2e
Use a custom scatter implementation
Out irreps 256x0e
Irreps after layer are Irreps()

============================================================
TRAINING WITH TIMING
============================================================
[Scaling] Node 2/6: local_device_ids = [0, 1, 2, 3]
[Scaling] Node 2: CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb1159.juwels:29540
[Scaling] Actual processes: 6
[Rank 2/6] JAX distributed initialized
[Rank 2] Local GPUs: 4, Total GPUs: 24
[Rank 2] Local devices: [CudaDevice(id=8), CudaDevice(id=9), CudaDevice(id=10), CudaDevice(id=11)]
[Rank 2] All devices: [CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2), CudaDevice(id=3), CudaDevice(id=4), CudaDevice(id=5), CudaDevice(id=6), CudaDevice(id=7), CudaDevice(id=8), CudaDevice(id=9), CudaDevice(id=10), CudaDevice(id=11), CudaDevice(id=12), CudaDevice(id=13), CudaDevice(id=14), CudaDevice(id=15), CudaDevice(id=16), CudaDevice(id=17), CudaDevice(id=18), CudaDevice(id=19), CudaDevice(id=20), CudaDevice(id=21), CudaDevice(id=22), CudaDevice(id=23)]
============================================================
SCALING TEST
============================================================
Devices: 24
Distributed: True
Rank: 2/6
Local devices: 4
Global devices: 24
============================================================
Capping edges and triplets. Beware of overflow, which is currently not being detected.
Estimated max. 900 edges.
[Allegro] Use two atom species for oxygen and hydroge.
[Allegro] Use default mask
Use a custom scatter implementation
Out irreps 300x0e+556x1o+256x1e+556x2e+384x2o+512x3o+256x3e
Irreps after layer are 128x1o+128x1e+128x2e+128x2o+128x3o+128x3e
Use a custom scatter implementation
Out irreps 256x0e+768x1o+896x2e
Irreps after layer are 128x1o+128x2e
Use a custom scatter implementation
Out irreps 256x0e
Irreps after layer are Irreps()

============================================================
TRAINING WITH TIMING
============================================================
[Scaling] Node 3/6: local_device_ids = [0, 1, 2, 3]
[Scaling] Node 3: CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb1159.juwels:29540
[Scaling] Actual processes: 6
[Rank 3/6] JAX distributed initialized
[Rank 3] Local GPUs: 4, Total GPUs: 24
[Rank 3] Local devices: [CudaDevice(id=12), CudaDevice(id=13), CudaDevice(id=14), CudaDevice(id=15)]
[Rank 3] All devices: [CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2), CudaDevice(id=3), CudaDevice(id=4), CudaDevice(id=5), CudaDevice(id=6), CudaDevice(id=7), CudaDevice(id=8), CudaDevice(id=9), CudaDevice(id=10), CudaDevice(id=11), CudaDevice(id=12), CudaDevice(id=13), CudaDevice(id=14), CudaDevice(id=15), CudaDevice(id=16), CudaDevice(id=17), CudaDevice(id=18), CudaDevice(id=19), CudaDevice(id=20), CudaDevice(id=21), CudaDevice(id=22), CudaDevice(id=23)]
============================================================
SCALING TEST
============================================================
Devices: 24
Distributed: True
Rank: 3/6
Local devices: 4
Global devices: 24
============================================================
Capping edges and triplets. Beware of overflow, which is currently not being detected.
Estimated max. 900 edges.
[Allegro] Use two atom species for oxygen and hydroge.
[Allegro] Use default mask
Use a custom scatter implementation
Out irreps 300x0e+556x1o+256x1e+556x2e+384x2o+512x3o+256x3e
Irreps after layer are 128x1o+128x1e+128x2e+128x2o+128x3o+128x3e
Use a custom scatter implementation
Out irreps 256x0e+768x1o+896x2e
Irreps after layer are 128x1o+128x2e
Use a custom scatter implementation
Out irreps 256x0e
Irreps after layer are Irreps()

============================================================
TRAINING WITH TIMING
============================================================
[Scaling] Node 0/6: local_device_ids = [0, 1, 2, 3]
[Scaling] Node 0: CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb1159.juwels:29540
[Scaling] Actual processes: 6
[Rank 0/6] JAX distributed initialized
[Rank 0] Local GPUs: 4, Total GPUs: 24
[Rank 0] Local devices: [CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2), CudaDevice(id=3)]
[Rank 0] All devices: [CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2), CudaDevice(id=3), CudaDevice(id=4), CudaDevice(id=5), CudaDevice(id=6), CudaDevice(id=7), CudaDevice(id=8), CudaDevice(id=9), CudaDevice(id=10), CudaDevice(id=11), CudaDevice(id=12), CudaDevice(id=13), CudaDevice(id=14), CudaDevice(id=15), CudaDevice(id=16), CudaDevice(id=17), CudaDevice(id=18), CudaDevice(id=19), CudaDevice(id=20), CudaDevice(id=21), CudaDevice(id=22), CudaDevice(id=23)]
============================================================
SCALING TEST
============================================================
Devices: 24
Distributed: True
Rank: 0/6
Local devices: 4
Global devices: 24
============================================================
Capping edges and triplets. Beware of overflow, which is currently not being detected.
Estimated max. 900 edges.
[Allegro] Use two atom species for oxygen and hydroge.
[Allegro] Use default mask
Use a custom scatter implementation
Out irreps 300x0e+556x1o+256x1e+556x2e+384x2o+512x3o+256x3e
Irreps after layer are 128x1o+128x1e+128x2e+128x2o+128x3o+128x3e
Use a custom scatter implementation
Out irreps 256x0e+768x1o+896x2e
Irreps after layer are 128x1o+128x2e
Use a custom scatter implementation
Out irreps 256x0e
Irreps after layer are Irreps()

============================================================
TRAINING WITH TIMING
============================================================
[Scaling] Node 4/6: local_device_ids = [0, 1, 2, 3]
[Scaling] Node 4: CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb1159.juwels:29540
[Scaling] Actual processes: 6
[Rank 4/6] JAX distributed initialized
[Rank 4] Local GPUs: 4, Total GPUs: 24
[Rank 4] Local devices: [CudaDevice(id=16), CudaDevice(id=17), CudaDevice(id=18), CudaDevice(id=19)]
[Rank 4] All devices: [CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2), CudaDevice(id=3), CudaDevice(id=4), CudaDevice(id=5), CudaDevice(id=6), CudaDevice(id=7), CudaDevice(id=8), CudaDevice(id=9), CudaDevice(id=10), CudaDevice(id=11), CudaDevice(id=12), CudaDevice(id=13), CudaDevice(id=14), CudaDevice(id=15), CudaDevice(id=16), CudaDevice(id=17), CudaDevice(id=18), CudaDevice(id=19), CudaDevice(id=20), CudaDevice(id=21), CudaDevice(id=22), CudaDevice(id=23)]
============================================================
SCALING TEST
============================================================
Devices: 24
Distributed: True
Rank: 4/6
Local devices: 4
Global devices: 24
============================================================
Capping edges and triplets. Beware of overflow, which is currently not being detected.
Estimated max. 900 edges.
[Allegro] Use two atom species for oxygen and hydroge.
[Allegro] Use default mask
Use a custom scatter implementation
Out irreps 300x0e+556x1o+256x1e+556x2e+384x2o+512x3o+256x3e
Irreps after layer are 128x1o+128x1e+128x2e+128x2o+128x3o+128x3e
Use a custom scatter implementation
Out irreps 256x0e+768x1o+896x2e
Irreps after layer are 128x1o+128x2e
Use a custom scatter implementation
Out irreps 256x0e
Irreps after layer are Irreps()

============================================================
TRAINING WITH TIMING
============================================================
[Scaling] Node 1/6: local_device_ids = [0, 1, 2, 3]
[Scaling] Node 1: CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb1159.juwels:29540
[Scaling] Actual processes: 6
[Rank 1/6] JAX distributed initialized
[Rank 1] Local GPUs: 4, Total GPUs: 24
[Rank 1] Local devices: [CudaDevice(id=4), CudaDevice(id=5), CudaDevice(id=6), CudaDevice(id=7)]
[Rank 1] All devices: [CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2), CudaDevice(id=3), CudaDevice(id=4), CudaDevice(id=5), CudaDevice(id=6), CudaDevice(id=7), CudaDevice(id=8), CudaDevice(id=9), CudaDevice(id=10), CudaDevice(id=11), CudaDevice(id=12), CudaDevice(id=13), CudaDevice(id=14), CudaDevice(id=15), CudaDevice(id=16), CudaDevice(id=17), CudaDevice(id=18), CudaDevice(id=19), CudaDevice(id=20), CudaDevice(id=21), CudaDevice(id=22), CudaDevice(id=23)]
============================================================
SCALING TEST
============================================================
Devices: 24
Distributed: True
Rank: 1/6
Local devices: 4
Global devices: 24
============================================================
Capping edges and triplets. Beware of overflow, which is currently not being detected.
Estimated max. 900 edges.
[Allegro] Use two atom species for oxygen and hydroge.
[Allegro] Use default mask
Use a custom scatter implementation
Out irreps 300x0e+556x1o+256x1e+556x2e+384x2o+512x3o+256x3e
Irreps after layer are 128x1o+128x1e+128x2e+128x2o+128x3o+128x3e
Use a custom scatter implementation
Out irreps 256x0e+768x1o+896x2e
Irreps after layer are 128x1o+128x2e
Use a custom scatter implementation
Out irreps 256x0e
Irreps after layer are Irreps()

============================================================
TRAINING WITH TIMING
============================================================
[Allegro] Use two atom species for oxygen and hydroge.
[Allegro] Use default mask
[Allegro] Use two atom species for oxygen and hydroge.
[Allegro] Use two atom species for oxygen and hydroge.
[Allegro] Use two atom species for oxygen and hydroge.
[Allegro] Use two atom species for oxygen and hydroge.
[Allegro] Use two atom species for oxygen and hydroge.
[Allegro] Use default mask
[Allegro] Use default mask
[Allegro] Use default mask
[Allegro] Use default mask
[Allegro] Use default mask
Use a custom scatter implementation
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Out irreps 300x0e+556x1o+256x1e+556x2e+384x2o+512x3o+256x3e
Irreps after layer are 128x1o+128x1e+128x2e+128x2o+128x3o+128x3e
Use a custom scatter implementation
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Use a custom scatter implementation
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Use a custom scatter implementation
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Use a custom scatter implementation
Out irreps 256x0e+768x1o+896x2e
Use a custom scatter implementation
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Use a custom scatter implementation
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Irreps after layer are 128x1o+128x2e
Use a custom scatter implementation
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Out irreps 300x0e+556x1o+256x1e+556x2e+384x2o+512x3o+256x3e
Out irreps 256x0e
Out irreps 300x0e+556x1o+256x1e+556x2e+384x2o+512x3o+256x3e
Out irreps 300x0e+556x1o+256x1e+556x2e+384x2o+512x3o+256x3e
Out irreps 300x0e+556x1o+256x1e+556x2e+384x2o+512x3o+256x3e
Out irreps 300x0e+556x1o+256x1e+556x2e+384x2o+512x3o+256x3e
Irreps after layer are Irreps()
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:183: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return asarray(x, dtype=self.dtype)
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:213: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in sum is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return _reduction(a, "sum", np.sum, lax.add, 0, preproc=_cast_to_numeric,
Irreps after layer are 128x1o+128x1e+128x2e+128x2o+128x3o+128x3e
Use a custom scatter implementation
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Irreps after layer are 128x1o+128x1e+128x2e+128x2o+128x3o+128x3e
Irreps after layer are 128x1o+128x1e+128x2e+128x2o+128x3o+128x3e
Use a custom scatter implementation
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Irreps after layer are 128x1o+128x1e+128x2e+128x2o+128x3o+128x3e
Use a custom scatter implementation
Irreps after layer are 128x1o+128x1e+128x2e+128x2o+128x3o+128x3e
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Use a custom scatter implementation
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Use a custom scatter implementation
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Out irreps 256x0e+768x1o+896x2e
Out irreps 256x0e+768x1o+896x2e
Out irreps 256x0e+768x1o+896x2e
Irreps after layer are 128x1o+128x2e
Out irreps 256x0e+768x1o+896x2e
Out irreps 256x0e+768x1o+896x2e
Use a custom scatter implementation
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Irreps after layer are 128x1o+128x2e
Use a custom scatter implementation
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Irreps after layer are 128x1o+128x2e
Use a custom scatter implementation
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Out irreps 256x0e
Out irreps 256x0e
Irreps after layer are 128x1o+128x2e
Out irreps 256x0e
Irreps after layer are 128x1o+128x2e
Use a custom scatter implementation
Irreps after layer are Irreps()
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Use a custom scatter implementation
Irreps after layer are Irreps()
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Irreps after layer are Irreps()
Out irreps 256x0e
Out irreps 256x0e
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:183: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return asarray(x, dtype=self.dtype)
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:213: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in sum is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return _reduction(a, "sum", np.sum, lax.add, 0, preproc=_cast_to_numeric,
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:183: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return asarray(x, dtype=self.dtype)
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:213: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in sum is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return _reduction(a, "sum", np.sum, lax.add, 0, preproc=_cast_to_numeric,
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:183: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return asarray(x, dtype=self.dtype)
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:213: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in sum is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return _reduction(a, "sum", np.sum, lax.add, 0, preproc=_cast_to_numeric,
Irreps after layer are Irreps()
Irreps after layer are Irreps()
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:183: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return asarray(x, dtype=self.dtype)
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:183: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return asarray(x, dtype=self.dtype)
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:213: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in sum is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return _reduction(a, "sum", np.sum, lax.add, 0, preproc=_cast_to_numeric,
/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:213: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in sum is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.
  return _reduction(a, "sum", np.sum, lax.add, 0, preproc=_cast_to_numeric,
[Force] Found precomputed forces.
[Force] Found precomputed forces.
[Force] Found precomputed forces.
[Force] Found precomputed forces.
[Force] Found precomputed forces.
[Force] Found precomputed forces.
[Force] Found precomputed forces.
[Force] Found precomputed forces.
[Force] Found precomputed forces.
[Force] Found precomputed forces.
[Force] Found precomputed forces.
[Force] Found precomputed forces.
[Epoch 0]:
	Average train loss: 467.56322
	Average val loss: 103.09393310546875
	Gradient norm: 38745.40625
	Elapsed time = 1.277 min
	Per-target losses:
		F | train loss: 467.56321868896487 | val loss: 103.09393310546875
[Epoch 0]:
	Average train loss: 467.56322
	Average val loss: 103.09393310546875
	Gradient norm: 38745.40625
	Elapsed time = 1.241 min
	Per-target losses:
		F | train loss: 467.56321868896487 | val loss: 103.09393310546875
[Epoch 0]:
	Average train loss: 467.56322
	Average val loss: 103.09393310546875
	Gradient norm: 38745.40625
	Elapsed time = 1.232 min
	Per-target losses:
		F | train loss: 467.56321868896487 | val loss: 103.09393310546875
[Epoch 0]:
	Average train loss: 467.56322
	Average val loss: 103.09393310546875
	Gradient norm: 38745.40625
	Elapsed time = 1.228 min
	Per-target losses:
		F | train loss: 467.56321868896487 | val loss: 103.09393310546875
[Epoch 0]:
	Average train loss: 467.56322
	Average val loss: 103.09393310546875
	Gradient norm: 38745.40625
	Elapsed time = 1.220 min
	Per-target losses:
		F | train loss: 467.56321868896487 | val loss: 103.09393310546875
[Epoch 0]:
	Average train loss: 467.56322
	Average val loss: 103.09393310546875
	Gradient norm: 38745.40625
	Elapsed time = 1.230 min
	Per-target losses:
		F | train loss: 467.56321868896487 | val loss: 103.09393310546875






[Epoch 1]:
	Average train loss: 106.21685
	Average val loss: 93.96709442138672
	Gradient norm: 342.8623046875
	Elapsed time = 0.152 min
	Per-target losses:
		F | train loss: 106.21685333251953 | val loss: 93.96709442138672
[Epoch 1]:
	Average train loss: 106.21685
	Average val loss: 93.96709442138672
	Gradient norm: 342.8623046875
	Elapsed time = 0.152 min
	Per-target losses:
		F | train loss: 106.21685333251953 | val loss: 93.96709442138672
[Epoch 1]:
	Average train loss: 106.21685
	Average val loss: 93.96709442138672
	Gradient norm: 342.8623046875
	Elapsed time = 0.152 min
	Per-target losses:
		F | train loss: 106.21685333251953 | val loss: 93.96709442138672
[Epoch 1]:
	Average train loss: 106.21685
	Average val loss: 93.96709442138672
	Gradient norm: 342.8623046875
	Elapsed time = 0.153 min
	Per-target losses:
		F | train loss: 106.21685333251953 | val loss: 93.96709442138672
[Epoch 1]:
	Average train loss: 106.21685
	Average val loss: 93.96709442138672
	Gradient norm: 342.8623046875
	Elapsed time = 0.153 min
	Per-target losses:
		F | train loss: 106.21685333251953 | val loss: 93.96709442138672
[Epoch 1]:
	Average train loss: 106.21685
	Average val loss: 93.96709442138672
	Gradient norm: 342.8623046875
	Elapsed time = 0.153 min
	Per-target losses:
		F | train loss: 106.21685333251953 | val loss: 93.96709442138672






[Epoch 2]:
	Average train loss: 92.82063
	Average val loss: 91.58837127685547
	Gradient norm: 46.56781005859375
	Elapsed time = 0.154 min
	Per-target losses:
		F | train loss: 92.82062683105468 | val loss: 91.58837127685547
[Epoch 2]:
	Average train loss: 92.82063
	Average val loss: 91.58837127685547
	Gradient norm: 46.56781005859375
	Elapsed time = 0.155 min
	Per-target losses:
		F | train loss: 92.82062683105468 | val loss: 91.58837127685547
[Epoch 2]:
	Average train loss: 92.82063
	Average val loss: 91.58837127685547
	Gradient norm: 46.56781005859375
	Elapsed time = 0.154 min
	Per-target losses:
		F | train loss: 92.82062683105468 | val loss: 91.58837127685547
[Epoch 2]:
	Average train loss: 92.82063
	Average val loss: 91.58837127685547
	Gradient norm: 46.56781005859375
	Elapsed time = 0.154 min
	Per-target losses:
		F | train loss: 92.82062683105468 | val loss: 91.58837127685547
[Epoch 2]:
	Average train loss: 92.82063
	Average val loss: 91.58837127685547
	Gradient norm: 46.56781005859375
	Elapsed time = 0.154 min
	Per-target losses:
		F | train loss: 92.82062683105468 | val loss: 91.58837127685547
[Epoch 2]:
	Average train loss: 92.82063
	Average val loss: 91.58837127685547
	Gradient norm: 46.56781005859375
	Elapsed time = 0.154 min
	Per-target losses:
		F | train loss: 92.82062683105468 | val loss: 91.58837127685547

[Training] 
Stage complete: train_loss=92.820627, val_loss=91.588371

Total training time: 116.90s

============================================================
SCALING TEST COMPLETE
============================================================





[Training] 
Stage complete: train_loss=92.820627, val_loss=91.588371

Total training time: 114.80s

============================================================
SCALING TEST COMPLETE
============================================================
[Training] 
Stage complete: train_loss=92.820627, val_loss=91.588371

Total training time: 114.07s

============================================================
SCALING TEST COMPLETE
============================================================
[Training] 
Stage complete: train_loss=92.820627, val_loss=91.588371

Total training time: 114.20s
Timing data saved to: /p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scaling_results/run_20260202_150203/timing_24dev_13191140_24dev.json

============================================================
SCALING TEST COMPLETE
============================================================
[Training] 
Stage complete: train_loss=92.820627, val_loss=91.588371

Total training time: 114.59s

============================================================
SCALING TEST COMPLETE
============================================================
[Training] 
Stage complete: train_loss=92.820627, val_loss=91.588371

Total training time: 113.61s

============================================================
SCALING TEST COMPLETE
============================================================
