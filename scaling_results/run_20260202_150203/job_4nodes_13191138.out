============================================================
Scaling Test Worker
============================================================
Job ID:         13191138
Nodes:          4
Start devices:  13
End devices:    16
Config:         /p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/config_scaling_test.yaml
Output dir:     /p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scaling_results/run_20260202_150203
============================================================

============================================================
Starting scaling tests from 13 to 16 devices
============================================================

------------------------------------------------------------
Testing with 13 device(s)...
------------------------------------------------------------
  Nodes needed:     4
  Device pattern:   split:3x4+1
  Running training...
2026-02-02 16:35:20.371925: E external/xla/xla/pjrt/distributed/client.cc:137] Failed to connect to distributed JAX controller: waited too long for some tasks to show up. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 5m.

Original runtime error: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier [type.googleapis.com/tensorflow.CoordinationServiceError='']
[Scaling] Node 0/4: local_device_ids = [0, 1, 2, 3]
[Scaling] Node 0: CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb0066.juwels:29538
[Scaling] Actual processes: 4
Traceback (most recent call last):
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 185, in <module>
2026-02-02 16:35:20.370793: E external/xla/xla/pjrt/distributed/client.cc:137] Failed to connect to distributed JAX controller: waited too long for some tasks to show up. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 5m.

Original runtime error: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier [type.googleapis.com/tensorflow.CoordinationServiceError='']
    _IS_DISTRIBUTED, _RANK, _WORLD_SIZE = _initialize_jax_distributed_scaling()
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 145, in _initialize_jax_distributed_scaling
    jax.distributed.initialize(
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 231, in initialize
2026-02-02 16:35:20.373268: E external/xla/xla/pjrt/distributed/client.cc:137] Failed to connect to distributed JAX controller: waited too long for some tasks to show up. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 5m.

Original runtime error: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier [type.googleapis.com/tensorflow.CoordinationServiceError='']
    global_state.initialize(coordinator_address, num_processes, process_id,
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 120, in initialize
    self.client.connect()
jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier
[Scaling] Node 2/4: local_device_ids = [0, 1, 2, 3]
[Scaling] Node 2: CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb0066.juwels:29538
[Scaling] Actual processes: 4
Traceback (most recent call last):
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 185, in <module>
[Scaling] Node 1/4: local_device_ids = [0, 1, 2, 3]
[Scaling] Node 1: CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb0066.juwels:29538
[Scaling] Actual processes: 4
Traceback (most recent call last):
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 185, in <module>
    _IS_DISTRIBUTED, _RANK, _WORLD_SIZE = _initialize_jax_distributed_scaling()
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 145, in _initialize_jax_distributed_scaling
    jax.distributed.initialize(
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 231, in initialize
    global_state.initialize(coordinator_address, num_processes, process_id,
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 120, in initialize
    self.client.connect()
jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier
    _IS_DISTRIBUTED, _RANK, _WORLD_SIZE = _initialize_jax_distributed_scaling()
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 145, in _initialize_jax_distributed_scaling
    jax.distributed.initialize(
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 231, in initialize
    global_state.initialize(coordinator_address, num_processes, process_id,
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 120, in initialize
    self.client.connect()
jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier
E0202 16:35:25.709208 3770244 coordination_service_agent.cc:892] Coordination agent is set to ERROR: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
2026-02-02 16:35:25.709281: E external/xla/xla/pjrt/distributed/client.cc:99] Coordination service agent in error status: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
2026-02-02 16:35:25.709972: F external/xla/xla/pjrt/distributed/client.h:82] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
srun: error: jwb0457: task 3: Aborted (core dumped)
srun: error: jwb0071: task 1: Terminated
srun: error: jwb0073: task 2: Terminated
srun: error: jwb0066: task 0: Terminated
srun: Force Terminated StepId=13191138.0
  Total time: 307.583263031s
  Throughput: 21.94 samples/s
  Done with 13 devices

------------------------------------------------------------
Testing with 14 device(s)...
------------------------------------------------------------
  Nodes needed:     4
  Device pattern:   split:3x4+2
  Running training...
2026-02-02 16:40:28.997353: E external/xla/xla/pjrt/distributed/client.cc:137] Failed to connect to distributed JAX controller: waited too long for some tasks to show up. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 5m.

Original runtime error: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier [type.googleapis.com/tensorflow.CoordinationServiceError='']
2026-02-02 16:40:28.997579: E external/xla/xla/pjrt/distributed/client.cc:137] Failed to connect to distributed JAX controller: waited too long for some tasks to show up. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 5m.

Original runtime error: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier [type.googleapis.com/tensorflow.CoordinationServiceError='']
2026-02-02 16:40:28.997576: E external/xla/xla/pjrt/distributed/client.cc:137] Failed to connect to distributed JAX controller: waited too long for some tasks to show up. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 5m.

Original runtime error: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier [type.googleapis.com/tensorflow.CoordinationServiceError='']
[Scaling] Node 0/4: local_device_ids = [0, 1, 2, 3]
[Scaling] Node 0: CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb0066.juwels:29538
[Scaling] Actual processes: 4
Traceback (most recent call last):
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 185, in <module>
[Scaling] Node 2/4: local_device_ids = [0, 1, 2, 3]
[Scaling] Node 2: CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb0066.juwels:29538
[Scaling] Actual processes: 4
    _IS_DISTRIBUTED, _RANK, _WORLD_SIZE = _initialize_jax_distributed_scaling()
Traceback (most recent call last):
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 185, in <module>
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 145, in _initialize_jax_distributed_scaling
    _IS_DISTRIBUTED, _RANK, _WORLD_SIZE = _initialize_jax_distributed_scaling()
    jax.distributed.initialize(
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 231, in initialize
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 145, in _initialize_jax_distributed_scaling
    global_state.initialize(coordinator_address, num_processes, process_id,
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 120, in initialize
    jax.distributed.initialize(
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 231, in initialize
    self.client.connect()
    global_state.initialize(coordinator_address, num_processes, process_id,
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 120, in initialize
jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier
    self.client.connect()
jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier
[Scaling] Node 1/4: local_device_ids = [0, 1, 2, 3]
[Scaling] Node 1: CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb0066.juwels:29538
[Scaling] Actual processes: 4
Traceback (most recent call last):
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 185, in <module>
    _IS_DISTRIBUTED, _RANK, _WORLD_SIZE = _initialize_jax_distributed_scaling()
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 145, in _initialize_jax_distributed_scaling
    jax.distributed.initialize(
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 231, in initialize
    global_state.initialize(coordinator_address, num_processes, process_id,
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 120, in initialize
    self.client.connect()
jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier
E0202 16:40:29.316310 3771166 coordination_service_agent.cc:892] Coordination agent is set to ERROR: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
2026-02-02 16:40:29.316375: E external/xla/xla/pjrt/distributed/client.cc:99] Coordination service agent in error status: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
2026-02-02 16:40:29.316426: F external/xla/xla/pjrt/distributed/client.h:82] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
srun: error: jwb0457: task 3: Aborted (core dumped)
srun: error: jwb0071: task 1: Terminated
srun: error: jwb0073: task 2: Terminated
srun: error: jwb0066: task 0: Terminated
srun: Force Terminated StepId=13191138.1
  Total time: 303.651583497s
  Throughput: 22.22 samples/s
  Done with 14 devices

------------------------------------------------------------
Testing with 15 device(s)...
------------------------------------------------------------
  Nodes needed:     4
  Device pattern:   split:3x4+3
  Running training...
2026-02-02 16:45:33.174498: E external/xla/xla/pjrt/distributed/client.cc:137] Failed to connect to distributed JAX controller: waited too long for some tasks to show up. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 5m.

Original runtime error: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier [type.googleapis.com/tensorflow.CoordinationServiceError='']
2026-02-02 16:45:33.174703: E external/xla/xla/pjrt/distributed/client.cc:137] Failed to connect to distributed JAX controller: waited too long for some tasks to show up. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 5m.

Original runtime error: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier [type.googleapis.com/tensorflow.CoordinationServiceError='']
2026-02-02 16:45:33.174731: E external/xla/xla/pjrt/distributed/client.cc:137] Failed to connect to distributed JAX controller: waited too long for some tasks to show up. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 5m.

Original runtime error: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier [type.googleapis.com/tensorflow.CoordinationServiceError='']
[Scaling] Node 1/4: local_device_ids = [0, 1, 2, 3]
[Scaling] Node 1: CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb0066.juwels:29538
[Scaling] Actual processes: 4
Traceback (most recent call last):
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 185, in <module>
    _IS_DISTRIBUTED, _RANK, _WORLD_SIZE = _initialize_jax_distributed_scaling()
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 145, in _initialize_jax_distributed_scaling
    jax.distributed.initialize(
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 231, in initialize
    global_state.initialize(coordinator_address, num_processes, process_id,
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 120, in initialize
    self.client.connect()
jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier
[Scaling] Node 0/4: local_device_ids = [0, 1, 2, 3]
[Scaling] Node 0: CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb0066.juwels:29538
[Scaling] Actual processes: 4
Traceback (most recent call last):
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 185, in <module>
    _IS_DISTRIBUTED, _RANK, _WORLD_SIZE = _initialize_jax_distributed_scaling()
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 145, in _initialize_jax_distributed_scaling
    jax.distributed.initialize(
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 231, in initialize
    global_state.initialize(coordinator_address, num_processes, process_id,
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 120, in initialize
    self.client.connect()
jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier
[Scaling] Node 2/4: local_device_ids = [0, 1, 2, 3]
[Scaling] Node 2: CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb0066.juwels:29538
[Scaling] Actual processes: 4
Traceback (most recent call last):
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 185, in <module>
    _IS_DISTRIBUTED, _RANK, _WORLD_SIZE = _initialize_jax_distributed_scaling()
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling.py", line 145, in _initialize_jax_distributed_scaling
    jax.distributed.initialize(
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 231, in initialize
    global_state.initialize(coordinator_address, num_processes, process_id,
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 120, in initialize
    self.client.connect()
jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 3/4.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:3


RPC: /tensorflow.CoordinationService/Barrier
E0202 16:45:34.533655 3772079 coordination_service_agent.cc:892] Coordination agent is set to ERROR: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
2026-02-02 16:45:34.533720: E external/xla/xla/pjrt/distributed/client.cc:99] Coordination service agent in error status: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
2026-02-02 16:45:34.533778: F external/xla/xla/pjrt/distributed/client.h:82] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
srun: error: jwb0457: task 3: Aborted (core dumped)
srun: error: jwb0071: task 1: Terminated
srun: error: jwb0073: task 2: Terminated
srun: error: jwb0066: task 0: Terminated
srun: Force Terminated StepId=13191138.2
  Total time: 305.156139192s
  Throughput: 22.11 samples/s
  Done with 15 devices

------------------------------------------------------------
Testing with 16 device(s)...
------------------------------------------------------------
  Nodes needed:     4
  Device pattern:   uniform:4
  Running training...
E0202 16:50:39.115301 3772998 coordination_service_agent.cc:892] Coordination agent is set to ERROR: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
2026-02-02 16:50:39.115374: E external/xla/xla/pjrt/distributed/client.cc:99] Coordination service agent in error status: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
2026-02-02 16:50:39.115416: F external/xla/xla/pjrt/distributed/client.h:82] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
srun: error: jwb0457: task 3: Aborted (core dumped)
srun: error: jwb0073: task 2: Terminated
srun: error: jwb0071: task 1: Terminated
srun: error: jwb0066: task 0: Terminated
srun: Force Terminated StepId=13191138.3
  Total time: 304.527830105s
  Throughput: 22.16 samples/s
  Done with 16 devices

============================================================
Scaling test complete for devices 13-16
Results saved to: /p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scaling_results/run_20260202_150203/timing_job_13191138.csv
============================================================
