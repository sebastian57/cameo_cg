============================================================
Scaling Test Worker (Fixed)
============================================================
Job ID:         13192061
Nodes:          6
Mode:           multi
Config:         /p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/config_scaling_test.yaml
Output dir:     /p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scaling_results/run_20260202_172828
============================================================

============================================================
Multi-node scaling test: 6 nodes, 24 devices
============================================================
  Running training on 6 nodes...
2026-02-02 20:12:48.197712: E external/xla/xla/pjrt/distributed/client.cc:137] Failed to connect to distributed JAX controller: waited too long for some tasks to show up. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 5m.

Original runtime error: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 4/6.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:4
/job:jax_worker/replica:0/task:5


RPC: /tensorflow.CoordinationService/Barrier [type.googleapis.com/tensorflow.CoordinationServiceError='']
2026-02-02 20:12:48.197074: E external/xla/xla/pjrt/distributed/client.cc:137] Failed to connect to distributed JAX controller: waited too long for some tasks to show up. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 5m.

Original runtime error: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 4/6.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:4
/job:jax_worker/replica:0/task:5


RPC: /tensorflow.CoordinationService/Barrier [type.googleapis.com/tensorflow.CoordinationServiceError='']
[Scaling] Multi-node mode: process 0/6
[Scaling] CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb0008.juwels:29461
2026-02-02 20:12:48.197240: E external/xla/xla/pjrt/distributed/client.cc:137] Failed to connect to distributed JAX controller: waited too long for some tasks to show up. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 5m.

Original runtime error: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 4/6.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:4
/job:jax_worker/replica:0/task:5


RPC: /tensorflow.CoordinationService/Barrier [type.googleapis.com/tensorflow.CoordinationServiceError='']
2026-02-02 20:12:48.197363: E external/xla/xla/pjrt/distributed/client.cc:137] Failed to connect to distributed JAX controller: waited too long for some tasks to show up. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 5m.

Original runtime error: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 4/6.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:4
/job:jax_worker/replica:0/task:5


RPC: /tensorflow.CoordinationService/Barrier [type.googleapis.com/tensorflow.CoordinationServiceError='']
Traceback (most recent call last):
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling_fixed.py", line 121, in <module>
    _IS_DISTRIBUTED, _RANK, _WORLD_SIZE = _initialize_jax_distributed()
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling_fixed.py", line 85, in _initialize_jax_distributed
    jax.distributed.initialize(
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 231, in initialize
    global_state.initialize(coordinator_address, num_processes, process_id,
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 120, in initialize
    self.client.connect()
jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 4/6.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:4
/job:jax_worker/replica:0/task:5


RPC: /tensorflow.CoordinationService/Barrier
[Scaling] Multi-node mode: process 1/6
[Scaling] CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb0008.juwels:29461
[Scaling] Multi-node mode: process 2/6
[Scaling] CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb0008.juwels:29461
Traceback (most recent call last):
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling_fixed.py", line 121, in <module>
[Scaling] Multi-node mode: process 3/6
[Scaling] CUDA_VISIBLE_DEVICES = 0,1,2,3
[Scaling] Coordinator: jwb0008.juwels:29461
Traceback (most recent call last):
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling_fixed.py", line 121, in <module>
Traceback (most recent call last):
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling_fixed.py", line 121, in <module>
    _IS_DISTRIBUTED, _RANK, _WORLD_SIZE = _initialize_jax_distributed()
    _IS_DISTRIBUTED, _RANK, _WORLD_SIZE = _initialize_jax_distributed()
    _IS_DISTRIBUTED, _RANK, _WORLD_SIZE = _initialize_jax_distributed()
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling_fixed.py", line 85, in _initialize_jax_distributed
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling_fixed.py", line 85, in _initialize_jax_distributed
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scripts/train_scaling_fixed.py", line 85, in _initialize_jax_distributed
    jax.distributed.initialize(
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 231, in initialize
    jax.distributed.initialize(
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 231, in initialize
    jax.distributed.initialize(
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 231, in initialize
    global_state.initialize(coordinator_address, num_processes, process_id,
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 120, in initialize
    global_state.initialize(coordinator_address, num_processes, process_id,
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 120, in initialize
    global_state.initialize(coordinator_address, num_processes, process_id,
  File "/p/software/default/stages/2025/software/jax/0.4.34-gpsfbf-2024a-CUDA-12/lib/python3.12/site-packages/jax/_src/distributed.py", line 120, in initialize
    self.client.connect()
    self.client.connect()
    self.client.connect()
jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 4/6.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:4
/job:jax_worker/replica:0/task:5


RPC: /tensorflow.CoordinationService/Barrier
jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 4/6.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:4
/job:jax_worker/replica:0/task:5


RPC: /tensorflow.CoordinationService/Barrier
jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Id: PjRT_Client_Connect. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
# of tasks that reached the barrier: 4/6.
The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
/job:jax_worker/replica:0/task:4
/job:jax_worker/replica:0/task:5


RPC: /tensorflow.CoordinationService/Barrier
E0202 20:12:48.926931 1773246 coordination_service_agent.cc:892] Coordination agent is set to ERROR: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
2026-02-02 20:12:48.926996: E external/xla/xla/pjrt/distributed/client.cc:99] Coordination service agent in error status: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
2026-02-02 20:12:48.927674: F external/xla/xla/pjrt/distributed/client.h:82] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
E0202 20:12:48.985870 2793892 coordination_service_agent.cc:892] Coordination agent is set to ERROR: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
2026-02-02 20:12:48.985941: E external/xla/xla/pjrt/distributed/client.cc:99] Coordination service agent in error status: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
2026-02-02 20:12:48.986289: F external/xla/xla/pjrt/distributed/client.h:82] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded

RPC: /tensorflow.CoordinationService/RegisterTask
srun: error: jwb0330: task 5: Aborted (core dumped)
srun: error: jwb0032: task 3: Terminated
srun: error: jwb0010: task 1: Terminated
srun: error: jwb0011: task 2: Terminated
srun: error: jwb0329: task 4: Aborted (core dumped)
srun: error: jwb0008: task 0: Terminated
srun: Force Terminated StepId=13192061.0
  Total time: 309.880229955s
  Throughput: 21.78 samples/s

============================================================
Scaling test complete
Results saved to: /p/project1/cameo/schmidt36/chemtrain-deploy/external/chemtrain/clean_code_base/scaling_results/run_20260202_172828/timing_job_13192061.csv
============================================================
